{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNiEXXParjssw2hIi4LFAs8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Algorit12/WebCrawlerForOnlineInflation/blob/master/Procesamiento_de_lenguaje_natural_usando_m%C3%A9todos_abstractivos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código usa librerias extractivas y Abstractivas para procesar lenguaje natural y generar resumenes funcionales y facilmente legibles para grandes volumenes de texto\n"
      ],
      "metadata": {
        "id": "WvXEXCcXL930"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W14utmrCQN1F",
        "outputId": "150f2237-67da-47c0-8e85-6f708ad5b164"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bertopic[all]\n",
            "  Downloading bertopic-0.15.0-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/143.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m143.4/143.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/143.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: bertopic 0.15.0 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from bertopic[all]) (1.22.4)\n",
            "Collecting hdbscan>=0.8.29 (from bertopic[all])\n",
            "  Downloading hdbscan-0.8.31.tar.gz (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install bertopic[all]\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSjYxErbTfzj"
      },
      "outputs": [],
      "source": [
        "!pip install fuzzywuzzy\n",
        "!pip install sklearn\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fS88-9Av2T02"
      },
      "outputs": [],
      "source": [
        "#Librerías de vectorización y similaridad para procesamiento de texto de manera vectorizada.\n",
        "import pandas as pd\n",
        "from fuzzywuzzy import fuzz\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qzNh-m3u18S"
      },
      "outputs": [],
      "source": [
        "#Librerías generales\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot     as plt\n",
        "import matplotlib.patches    as mpatches\n",
        "import seaborn               as sns\n",
        "import sklearn.metrics       as Metrics\n",
        "from google.colab import drive\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5caf654-c18a-4541-934e-a28b0d5f0b1c"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#delcarando part_dir y nombre de archivos\n",
        "path_dir = '/content/drive/MyDrive/grupo_express/word/'"
      ],
      "metadata": {
        "id": "A9QhWiVnNViw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def read_txt(file_path):\n",
        "    with open(file_path, 'r', encoding='latin-1') as file:\n",
        "        text = file.read()\n",
        "    return text"
      ],
      "metadata": {
        "id": "rTTF0SuwQm5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rake_nltk"
      ],
      "metadata": {
        "id": "Cit6u3p41gkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "D9mlun727u3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rake_nltk import Rake\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub('[^a-záéíóúñ ]', '', text)\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "def extract_keywords(text, top_n=50):\n",
        "    r = Rake(min_length=1, max_length=100)\n",
        "    r.extract_keywords_from_text(text)\n",
        "    ranked_phrases = r.get_ranked_phrases()\n",
        "    keywords = ranked_phrases[:top_n]  # ajustar el número de Keywords\n",
        "    return keywords\n",
        "\n",
        "def process_txt_files(file_paths):\n",
        "    texts = []\n",
        "    for file_path in file_paths:\n",
        "        with open(file_path, 'r', encoding='latin-1') as file:\n",
        "            text = file.read()\n",
        "            preprocessed_text = preprocess_text(text)\n",
        "            texts.append(preprocessed_text)\n",
        "\n",
        "    if len(texts) == 1:\n",
        "        keywords = extract_keywords(texts[0])\n",
        "        return keywords\n",
        "    else:\n",
        "        raise ValueError(\"Keyword extraction is only supported for a single text.\")\n",
        "\n",
        "# Update file paths and extensions\n",
        "files = [\"Taller Entrevista Depto IT_2023-06-06.txt\"]\n",
        "file_paths = [path_dir + file for file in files]  # Construct the complete file paths\n",
        "\n",
        "keywords = process_txt_files(file_paths)\n",
        "print(keywords)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4QMz2Oay1eaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords"
      ],
      "metadata": {
        "id": "texlCLqV4Di3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rake_nltk import Rake\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub('[^a-záéíóúñ ]', '', text)\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "def extract_keywords(text, top_n=50):\n",
        "    r = Rake(min_length=1, max_length=100)\n",
        "    r.extract_keywords_from_text(text)\n",
        "    ranked_phrases = r.get_ranked_phrases()\n",
        "    keywords = ranked_phrases[:top_n]  # Adjust the number of keywords here\n",
        "    return keywords\n",
        "\n",
        "def process_txt_files(file_paths):\n",
        "    texts = []\n",
        "    for file_path in file_paths:\n",
        "        with open(file_path, 'r', encoding='latin-1') as file:\n",
        "            text = file.read()\n",
        "            preprocessed_text = preprocess_text(text)\n",
        "            texts.append(preprocessed_text)\n",
        "\n",
        "    if len(texts) == 1:\n",
        "        keywords = extract_keywords(texts[0])\n",
        "        return keywords\n",
        "    else:\n",
        "        raise ValueError(\"Keyword extraction is only supported for a single text.\")\n",
        "\n",
        "# Update file paths and extensions\n",
        "files = [\"Director Gestion Humana_2023-06-07.docx\"]\n",
        "file_paths = [path_dir + file for file in files]  # Construct the complete file paths\n",
        "\n",
        "keywords = process_txt_files(file_paths)\n",
        "print(keywords)\n",
        "\n"
      ],
      "metadata": {
        "id": "Gd6Il4f6wZuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = process_txt_files(file_paths)\n",
        "\n",
        "# dividir el texto y extraer las keywords\n",
        "all_keywords = [keyword for keywords_string in keywords for keyword in keywords_string.split()]\n",
        "\n",
        "print(all_keywords)"
      ],
      "metadata": {
        "id": "796AJF_2tjow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the BART model for text generation\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Update file paths and extensions\n",
        "files = [\"Director Gestion Humana_2023-06-07.docx.txt\"]\n",
        "file_paths = [path_dir + file for file in files]  # Construct the complete file paths\n",
        "\n",
        "# Read the file contents\n",
        "texts = []\n",
        "for file_path in file_paths:\n",
        "    with open(file_path, \"r\", encoding='latin-1') as file:\n",
        "        text = file.read()\n",
        "        texts.append(text)\n",
        "\n",
        "# Generate abstractive summaries for each chunk of text\n",
        "summaries = []\n",
        "max_length = 200\n",
        "min_length = 30\n",
        "chunk_size = 2000  # Adjust the chunk size as needed\n",
        "\n",
        "for text in texts:\n",
        "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "    for chunk in chunks:\n",
        "        summary = summarizer(chunk, max_length=max_length, min_length=min_length, do_sample=True)\n",
        "        summaries.append(summary[0]['summary_text'])\n",
        "\n",
        "# Print the summaries\n",
        "for i, summary in enumerate(summaries):\n",
        "    print(f\"Summary {i+1}: {summary}\")"
      ],
      "metadata": {
        "id": "5AJdGOIIGDsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# cargar el modelo BART\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "\n",
        "files = [\"Director Ops_2023-06-07.txt\"]\n",
        "file_paths = [path_dir + file for file in files]  # Construct the complete file paths\n",
        "\n",
        "texts = []\n",
        "for file_path in file_paths:\n",
        "    with open(file_path, \"r\", encoding='latin-1') as file:\n",
        "        text = file.read()\n",
        "        texts.append(text)\n",
        "\n",
        "# Generar resumenes con técnica abstractiva\n",
        "summaries = []\n",
        "max_length = 200\n",
        "min_length = 30\n",
        "chunk_size = 2000\n",
        "\n",
        "for text in texts:\n",
        "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "    for chunk in chunks:\n",
        "        summary = summarizer(chunk, max_length=max_length, min_length=min_length, do_sample=True)\n",
        "        summaries.append(summary[0]['summary_text'])\n",
        "\n",
        "\n",
        "for i, summary in enumerate(summaries):\n",
        "    print(f\"Summary {i+1}: {summary}\")"
      ],
      "metadata": {
        "id": "ryyC33-4zbd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# cargamos BART para generalizar\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "files = [\"Gerencia IT_2023-06-07.txt\"]\n",
        "file_paths = [path_dir + file for file in files]  # Construct the complete file paths\n",
        "\n",
        "\n",
        "texts = []\n",
        "for file_path in file_paths:\n",
        "    with open(file_path, \"r\", encoding='latin-1') as file:\n",
        "        text = file.read()\n",
        "        texts.append(text)\n",
        "\n",
        "# se genera resumenes abstractivos de cada parte del texto\n",
        "summaries = []\n",
        "max_length = 200\n",
        "min_length = 30\n",
        "chunk_size = 2000  # ajustamos el tamaño de los \"pedazos\" de texto, según se requiera\n",
        "\n",
        "for text in texts:\n",
        "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "    for chunk in chunks:\n",
        "        summary = summarizer(chunk, max_length=max_length, min_length=min_length, do_sample=True)\n",
        "        summaries.append(summary[0]['summary_text'])\n",
        "\n",
        "\n",
        "for i, summary in enumerate(summaries):\n",
        "    print(f\"Summary {i+1}: {summary}\")"
      ],
      "metadata": {
        "id": "zp383v6uCH9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# cargamos el modelo BART para generalizar\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Update file paths and extensions\n",
        "files = [\"Control Interno_2023-06-07.txt\"]\n",
        "file_paths = [path_dir + file for file in files]  # Construct the complete file paths\n",
        "\n",
        "\n",
        "texts = []\n",
        "for file_path in file_paths:\n",
        "    with open(file_path, \"r\", encoding='latin-1') as file:\n",
        "        text = file.read()\n",
        "        texts.append(text)\n",
        "\n",
        "# se genera resumenes abstractivos para cada parte del texto\n",
        "summaries = []\n",
        "max_length = 150\n",
        "min_length = 30\n",
        "chunk_size = 2000  # Aquí puedes ajustar el tamaño del texto qué se va a procesar  resumiendo de una manera abstractiva\n",
        "\n",
        "for text in texts:\n",
        "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "    for chunk in chunks:\n",
        "        summary = summarizer(chunk, max_length=max_length, min_length=min_length, do_sample=True)\n",
        "        summaries.append(summary[0]['summary_text'])\n",
        "\n",
        "\n",
        "for i, summary in enumerate(summaries):\n",
        "    print(f\"Summary {i+1}: {summary}\")"
      ],
      "metadata": {
        "id": "mx-Pv40yz9_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "toDewjo2KaXx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}